#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PriorityMax - FAANG-Level Production RL Environment (STABLE VERSION)
---------------------------------------------------------------------
‚úÖ FIXED: Bounded rewards with tanh() normalization
‚úÖ FIXED: Observation normalization for stable training
‚úÖ FIXED: Reward scaling to prevent value explosion
‚úÖ FIXED: All hyperparameters tuned for stability
‚úÖ MAINTAINS: All complex features (workloads, failures, multi-region)
"""

from __future__ import annotations

import os
import sys
import time
import json
import math
import uuid
import random
import logging
import pathlib
import threading
import signal
import atexit
from dataclasses import dataclass, field, asdict
from typing import Any, Dict, List, Tuple, Optional, Callable, Union
from collections import deque, defaultdict
from datetime import datetime, timedelta
from enum import Enum
import statistics

import numpy as np

# Logging
LOG = logging.getLogger("prioritymax.ml.real_env")
LOG.setLevel(os.getenv("PRIORITYMAX_ENV_LOG", "INFO"))
_handler = logging.StreamHandler(sys.stdout)
_handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s"))
if not LOG.handlers:
    LOG.addHandler(_handler)

# Paths
BASE_DIR = pathlib.Path(__file__).resolve().parents[2]
AUDIT_LOG_DIR = BASE_DIR / "logs"
AUDIT_LOG_DIR.mkdir(parents=True, exist_ok=True)

# =============================================================================
# ENUMS AND CONSTANTS
# =============================================================================

class WorkloadType(Enum):
    """Real-world workload patterns"""
    ECOMMERCE = "ecommerce"
    SOCIAL_MEDIA = "social_media"
    STREAMING = "streaming"
    API_BACKEND = "api_backend"
    BATCH_PROCESSING = "batch"
    GAMING = "gaming"

class FailureMode(Enum):
    """Worker failure scenarios"""
    CRASH = "crash"
    BYZANTINE = "byzantine"
    SLOW = "slow"
    NETWORK = "network"
    OOM = "oom"

class WorkerTier(Enum):
    """Worker types with cost/performance tradeoffs"""
    SPOT = "spot"
    ON_DEMAND = "on_demand"
    RESERVED = "reserved"

# =============================================================================
# OBSERVATION NORMALIZATION (CRITICAL FOR STABILITY)
# =============================================================================

class ObservationNormalizer:
    """
    Normalizes observations to ~N(0,1) range for stable training.
    CRITICAL: Prevents gradient explosion from large raw values.
    """
    def __init__(self, obs_dim: int = 12):
        self.obs_dim = obs_dim
        
        # Expected ranges for each dimension (for normalization)
        self.scales = np.array([
            5000.0,   # queue_length (0-50k)
            100.0,    # worker_count (0-500)
            500.0,    # avg_latency_ms (0-10k)
            1000.0,   # p95_latency_ms (0-20k)
            1.0,      # success_rate (0-1)
            100.0,    # arrival_rate (0-200)
            1.0,      # cpu (0-1)
            1.0,      # mem (0-1)
            1500.0,   # p99_latency_ms (0-30k)
            1.0,      # cost_rate (0-10)
            1.0,      # worker_health (0-1)
            200.0     # queue_growth_rate (-500 to +500)
        ], dtype=np.float32)[:obs_dim]
        
        self.offsets = np.zeros(obs_dim, dtype=np.float32)
        self.offsets[11] = 0.0  # queue_growth_rate can be negative
        
    def normalize(self, obs: np.ndarray) -> np.ndarray:
        """Normalize observation to ~[-1, 1] range"""
        normalized = (obs - self.offsets) / (self.scales + 1e-8)
        return np.clip(normalized, -5.0, 5.0)  # Safety clip

# =============================================================================
# REWARD NORMALIZATION (CRITICAL FOR STABILITY)
# =============================================================================

class RewardNormalizer:
    """
    Running normalization of rewards to maintain stable value predictions.
    Uses Welford's online algorithm for numerical stability.
    """
    def __init__(self, clip_range: float = 10.0):
        self.mean = 0.0
        self.var = 1.0
        self.count = 0
        self.clip_range = clip_range
        
    def normalize(self, reward: float) -> float:
        """Normalize reward using running statistics"""
        # Update statistics
        self.count += 1
        delta = reward - self.mean
        self.mean += delta / self.count
        delta2 = reward - self.mean
        self.var += (delta * delta2 - self.var) / self.count
        
        # Normalize
        std = np.sqrt(self.var + 1e-8)
        normalized = (reward - self.mean) / std
        
        # Clip to prevent extreme values
        return float(np.clip(normalized, -self.clip_range, self.clip_range))

# =============================================================================
# THREAD-SAFE AUDIT LOGGER
# =============================================================================

class AuditLogger:
    def __init__(self, log_dir: pathlib.Path = AUDIT_LOG_DIR):
        self.log_file = log_dir / "real_env_audit.jsonl"
        self.lock = threading.Lock()
        self.enabled = True
        
    def log(self, event: Dict[str, Any]):
        if not self.enabled:
            return
        try:
            with self.lock:
                if "timestamp" not in event:
                    event["timestamp"] = datetime.utcnow().isoformat() + "Z"
                with open(self.log_file, "a", encoding="utf-8") as f:
                    f.write(json.dumps(event, default=str) + "\n")
        except Exception as e:
            LOG.debug("Audit log write failed: %s", e)

_AUDIT_LOGGER = AuditLogger()

def write_audit_event(payload: Dict[str, Any]):
    _AUDIT_LOGGER.log(payload)

# =============================================================================
# CONFIGURATION (STABLE HYPERPARAMETERS)
# =============================================================================

@dataclass
class EnvConfig:
    """FAANG-level environment configuration with STABLE defaults"""
    
    # Mode
    mode: str = "sim"
    
    # Observation/Action dimensions
    obs_dim: Optional[int] = 12
    act_dim: int = 3
    
    # === WORKLOAD (TUNED FOR STABLE TRAINING) ===
    workload_type: str = "ecommerce"
    base_arrival_rate: float = 10.0
    peak_arrival_multiplier: float = 5.0          # Reduced from 8.0
    flash_crowd_probability: float = 0.0001         # Reduced from 0.03
    flash_crowd_duration_steps: int = 15          # Reduced from 30
    flash_crowd_multiplier: float = 2.0          # Reduced from 15.0
    
    # Diurnal patterns
    diurnal_amplitude: float = 0.6                # Reduced from 0.8
    diurnal_peak_hour: float = 14.0
    weekly_pattern: bool = True
    seasonal_variation: float = 0.2               # Reduced from 0.3
    
    # Queue and latency
    max_queue_size: int = 200000
    base_latency_ms: float = 50.0
    latency_per_queue_item: float = 0.05
    network_latency_ms: float = 20.0
    
    # Workers
    min_workers: int = 5
    max_workers: int = 500
    worker_startup_time_steps: int = 3
    worker_shutdown_time_steps: int = 1
    spot_instance_probability: float = 0.7
    spot_preemption_rate: float = 0.001           # Reduced from 0.002
    
    worker_efficiency_mean: float = 1.0
    worker_efficiency_std: float = 0.2            # Reduced from 0.25
    worker_capacity_per_sec: float = 10.0
    
    # Failures (TUNED FOR STABILITY)
    base_failure_rate: float = 0.0005              # Reduced from 0.005
    overload_failure_multiplier: float = 2.0      # Reduced from 3.0
    cascade_failure_probability: float = 0.001    # Reduced from 0.01
    byzantine_failure_rate: float = 0.0005        # Reduced from 0.001
    network_partition_probability: float = 0.002  # Reduced from 0.005
    
    # Cost model
    cost_on_demand_per_sec: float = 0.001
    cost_spot_per_sec: float = 0.0003
    cost_reserved_per_sec: float = 0.0007
    cost_sla_violation: float = 25.0              # Reduced from 50.0
    cost_data_transfer_per_gb: float = 0.01
    
    # SLA targets
    sla_latency_p50_ms: float = 500.0
    sla_latency_p95_ms: float = 2000.0
    sla_latency_p99_ms: float = 4000.0
    sla_success_rate: float = 0.995
    sla_availability: float = 0.999
    
    # Actions
    max_scale_delta: int = 20
    action_cooldown_seconds: float = 3.0
    
    # === REWARD WEIGHTS (TUNED FOR STABLE TRAINING) ===
    reward_latency_weight: float = 2.0            # Reduced from 5.0
    reward_cost_weight: float = 1.0               # Reduced from 2.0
    reward_availability_weight: float = 3.0       # Reduced from 10.0
    reward_efficiency_weight: float = 0.5         # Reduced from 1.0
    reward_stability_weight: float = 0.3          # Reduced from 0.5
    reward_sla_bonus: float = 2.0                 # Reduced from 5.0
    
    # Features
    enable_multi_region: bool = True
    enable_canary_deployment: bool = True
    enable_auto_remediation: bool = True
    enable_predictive_scaling: bool = False
    enable_drift_tracking: bool = True
    drift_window_size: int = 2000
    enable_audit_logging: bool = True
    
    # Circuit breaker
    enable_circuit_breaker: bool = True
    circuit_breaker_threshold: int = 500
    circuit_breaker_window_seconds: int = 120
    
    # Seed
    seed: Optional[int] = None
    
    # Live mode (not used in training)
    redis_url: Optional[str] = None
    dry_run: bool = True

# =============================================================================
# WORKLOAD GENERATOR
# =============================================================================

class WorkloadGenerator:
    """Generate realistic workload patterns"""
    
    def __init__(self, config: EnvConfig, rng: np.random.RandomState):
        self.cfg = config
        self.rng = rng
        self.workload_type = WorkloadType(config.workload_type)
        
        self.sim_time_minutes = 0.0
        self.day_of_week = 1
        self.in_flash_crowd = False
        self.flash_crowd_remaining = 0
        
    def get_arrival_rate(self) -> float:
        """Compute current arrival rate with all modulations"""
        rate = self.cfg.base_arrival_rate
        
        # Diurnal pattern
        hour_of_day = (self.sim_time_minutes / 60.0) % 24.0
        diurnal_factor = 1.0 + self.cfg.diurnal_amplitude * math.sin(
            ((hour_of_day - self.cfg.diurnal_peak_hour) / 24.0) * 2 * math.pi
        )
        rate *= diurnal_factor
        
        # Weekly pattern
        if self.cfg.weekly_pattern:
            if self.day_of_week in [6, 7]:
                if self.workload_type == WorkloadType.ECOMMERCE:
                    rate *= 1.3
                elif self.workload_type == WorkloadType.STREAMING:
                    rate *= 1.5
            else:
                if self.workload_type == WorkloadType.API_BACKEND:
                    rate *= 1.2
        
        # Workload-specific patterns
        if self.workload_type == WorkloadType.ECOMMERCE:
            if self.rng.random() < 0.0001:
                rate *= 50.0
                LOG.warning("üõçÔ∏è BLACK FRIDAY EVENT!")
        
        elif self.workload_type == WorkloadType.SOCIAL_MEDIA:
            if self.rng.random() < 0.01:
                viral_multiplier = self.rng.uniform(5.0, 20.0)
                rate *= viral_multiplier
                LOG.info("üî• VIRAL EVENT! Traffic: %.1fx", viral_multiplier)
        
        elif self.workload_type == WorkloadType.STREAMING:
            if 18 <= hour_of_day <= 23:
                rate *= 2.5
        
        elif self.workload_type == WorkloadType.GAMING:
            if 19 <= hour_of_day <= 22:
                rate *= 4.0
        
        # Flash crowd
        if not self.in_flash_crowd:
            if self.rng.random() < self.cfg.flash_crowd_probability:
                self.in_flash_crowd = True
                self.flash_crowd_remaining = self.cfg.flash_crowd_duration_steps
                LOG.warning("‚ö° FLASH CROWD started!")
        
        if self.in_flash_crowd:
            rate *= self.cfg.flash_crowd_multiplier
            self.flash_crowd_remaining -= 1
            if self.flash_crowd_remaining <= 0:
                self.in_flash_crowd = False
                LOG.info("‚úÖ Flash crowd ended")
        
        # Random noise
        noise = self.rng.lognormal(0, 0.2)
        rate *= noise
        
        return max(0.0, rate)
    
    def advance_time(self, minutes: float = 1.0):
        """Advance simulation time"""
        self.sim_time_minutes += minutes
        if self.sim_time_minutes >= 1440:
            self.sim_time_minutes = 0
            self.day_of_week = (self.day_of_week % 7) + 1

# =============================================================================
# WORKER POOL
# =============================================================================

@dataclass
class Worker:
    """Individual worker with state"""
    id: str
    tier: WorkerTier
    efficiency: float
    startup_remaining: int
    is_healthy: bool
    failure_mode: Optional[FailureMode]
    tasks_processed: int
    created_at: float

class WorkerPool:
    """Manage heterogeneous worker pool"""
    
    def __init__(self, config: EnvConfig, rng: np.random.RandomState):
        self.cfg = config
        self.rng = rng
        self.workers: List[Worker] = []
        self.next_worker_id = 0
        
    def add_workers(self, count: int) -> int:
        """Add new workers"""
        added = 0
        for _ in range(count):
            if len(self.workers) >= self.cfg.max_workers:
                break
            
            tier = WorkerTier.SPOT if self.rng.random() < self.cfg.spot_instance_probability else WorkerTier.ON_DEMAND
            efficiency = max(0.1, self.rng.normal(self.cfg.worker_efficiency_mean, self.cfg.worker_efficiency_std))
            
            worker = Worker(
                id=f"worker_{self.next_worker_id}",
                tier=tier,
                efficiency=efficiency,
                startup_remaining=self.cfg.worker_startup_time_steps,
                is_healthy=True,
                failure_mode=None,
                tasks_processed=0,
                created_at=time.time()
            )
            
            self.workers.append(worker)
            self.next_worker_id += 1
            added += 1
        
        return added
    
    def remove_workers(self, count: int) -> int:
        """Remove workers"""
        to_remove = min(count, len(self.workers))
        self.workers.sort(key=lambda w: (w.is_healthy, w.tier != WorkerTier.SPOT, -w.tasks_processed))
        self.workers = self.workers[to_remove:]
        return to_remove
    
    def update_workers(self, overload_factor: float):
        """Update worker states"""
        for w in self.workers:
            if w.startup_remaining > 0:
                w.startup_remaining -= 1
        
        # Spot preemptions
        for w in self.workers:
            if w.tier == WorkerTier.SPOT:
                if self.rng.random() < self.cfg.spot_preemption_rate:
                    w.is_healthy = False
                    w.failure_mode = FailureMode.CRASH
        
        # Failure injection
        failure_rate = self.cfg.base_failure_rate * (1.0 + overload_factor * self.cfg.overload_failure_multiplier)
        
        for w in self.workers:
            if not w.is_healthy:
                continue
            
            if self.rng.random() < failure_rate:
                failure_roll = self.rng.random()
                if failure_roll < 0.05:
                    w.failure_mode = FailureMode.BYZANTINE
                elif failure_roll < 0.3:
                    w.failure_mode = FailureMode.SLOW
                elif failure_roll < 0.5:
                    w.failure_mode = FailureMode.NETWORK
                elif failure_roll < 0.7:
                    w.failure_mode = FailureMode.OOM
                else:
                    w.failure_mode = FailureMode.CRASH
                
                w.is_healthy = False
        
        self.workers = [w for w in self.workers if w.failure_mode != FailureMode.CRASH]
    
    def get_effective_capacity(self) -> float:
        """Compute total processing capacity"""
        capacity = 0.0
        for w in self.workers:
            if not w.is_healthy:
                if w.failure_mode == FailureMode.SLOW:
                    capacity += w.efficiency * 0.2
            elif w.startup_remaining > 0:
                capacity += w.efficiency * 0.5
            else:
                capacity += w.efficiency
        return capacity * self.cfg.worker_capacity_per_sec
    
    def get_cost_per_second(self) -> float:
        """Compute current cost rate"""
        cost = 0.0
        for w in self.workers:
            if w.tier == WorkerTier.SPOT:
                cost += self.cfg.cost_spot_per_sec
            elif w.tier == WorkerTier.ON_DEMAND:
                cost += self.cfg.cost_on_demand_per_sec
            else:
                cost += self.cfg.cost_reserved_per_sec
        return cost
    
    def count_healthy(self) -> int:
        return sum(1 for w in self.workers if w.is_healthy and w.startup_remaining == 0)
    
    def count_total(self) -> int:
        return len(self.workers)

# =============================================================================
# CIRCUIT BREAKER (FIXED AUTO-RECOVERY)
# =============================================================================

class CircuitBreaker:
    def __init__(self, threshold: int = 50, window_seconds: int = 120):
        self.threshold = threshold
        self.window_seconds = window_seconds
        self.failures = deque()
        self.successes = deque()
        self.state = "CLOSED"
        self.last_state_change = time.time()
        self.lock = threading.Lock()
        
    def record_failure(self):
        with self.lock:
            now = time.time()
            self.failures.append(now)
            
            cutoff = now - self.window_seconds
            self.failures = deque([t for t in self.failures if t > cutoff])
            
            recent_successes = sum(1 for t in self.successes if t > cutoff)
            failure_rate = len(self.failures) / max(1, len(self.failures) + recent_successes)
            
            if len(self.failures) >= self.threshold and failure_rate > 0.7:
                if self.state == "CLOSED":
                    self.state = "OPEN"
                    self.last_state_change = now
    
    def record_success(self):
        with self.lock:
            now = time.time()
            self.successes.append(now)
            cutoff = now - self.window_seconds
            self.successes = deque([t for t in self.successes if t > cutoff])
            
            # Auto-recovery: if enough successes, close circuit
            if self.state == "OPEN" or self.state == "HALF_OPEN":
                if len(self.successes) >= 10:
                    self.state = "CLOSED"
    
    def can_execute(self) -> bool:
        with self.lock:
            now = time.time()
            
            if self.state == "OPEN" and (now - self.last_state_change) > self.window_seconds:
                self.state = "HALF_OPEN"
            
            return self.state != "OPEN"

# =============================================================================
# DRIFT TRACKER
# =============================================================================

class DriftTracker:
    def __init__(self, window_size: int = 2000):
        self.window_size = window_size
        self.obs_buffer = deque(maxlen=window_size)
        self.reward_buffer = deque(maxlen=window_size)
        self.baseline_obs_stats = None
        self.baseline_reward_stats = None
        self.lock = threading.Lock()
        
    def add_sample(self, obs: np.ndarray, reward: float):
        with self.lock:
            self.obs_buffer.append(obs.copy() if isinstance(obs, np.ndarray) else np.array(obs))
            self.reward_buffer.append(float(reward))
            
            if len(self.obs_buffer) == self.window_size and self.baseline_obs_stats is None:
                self._compute_baseline()
    
    def _compute_baseline(self):
        obs_array = np.array(list(self.obs_buffer))
        reward_array = np.array(list(self.reward_buffer))
        
        self.baseline_obs_stats = {
            "mean": np.mean(obs_array, axis=0),
            "std": np.std(obs_array, axis=0)
        }
        
        self.baseline_reward_stats = {
            "mean": float(np.mean(reward_array)),
            "std": float(np.std(reward_array))
        }
    
    def get_drift_score(self) -> Optional[float]:
        with self.lock:
            if self.baseline_reward_stats is None or len(self.reward_buffer) < self.window_size:
                return None
            
            current_mean = np.mean(list(self.reward_buffer))
            baseline_mean = self.baseline_reward_stats["mean"]
            baseline_std = self.baseline_reward_stats["std"]
            
            return float(abs(current_mean - baseline_mean) / (baseline_std + 1e-8))

# =============================================================================
# OBSERVATION/ACTION SPACES
# =============================================================================

def get_action_space() -> Dict:
    """Action: [delta_workers, throttle_scale, priority_bias]"""
    return {
        "low": np.array([-20.0, 0.0, -2.0], dtype=np.float32),
        "high": np.array([20.0, 1.0, 2.0], dtype=np.float32),
        "shape": (3,)
    }

def get_observation_space(obs_dim: int = 12) -> Dict:
    """Extended observation space"""
    return {
        "low": np.zeros(obs_dim, dtype=np.float32),
        "high": np.ones(obs_dim, dtype=np.float32) * 1e6,
        "shape": (obs_dim,)
    }

# =============================================================================
# BASE ENVIRONMENT
# =============================================================================

class RealEnvBase:
    """Base environment with stable training features"""
    
    def __init__(self, config: EnvConfig):
        self.cfg = config
        self.random = random.Random(config.seed)
        self.npr = np.random.RandomState(config.seed)
        
        if config.obs_dim is None:
            config.obs_dim = 12
        
        self.action_space = get_action_space()
        self.observation_space = get_observation_space(config.obs_dim)
        
        self.current_time = time.time()
        self.last_action_time = 0.0
        self.step_count = 0
        self.episode_count = 0
        self.closed = False
        
        # Normalizers (CRITICAL FOR STABILITY)
        self.obs_normalizer = ObservationNormalizer(config.obs_dim)
        self.reward_normalizer = RewardNormalizer(clip_range=10.0)
        
        self.circuit_breaker = CircuitBreaker(
            config.circuit_breaker_threshold,
            config.circuit_breaker_window_seconds
        ) if config.enable_circuit_breaker else None
        
        self.drift_tracker = DriftTracker(
            config.drift_window_size
        ) if config.enable_drift_tracking else None
        
        self.metrics_history: List[Dict[str, Any]] = []
        self.sla_violations = 0
        self.total_cost = 0.0
        self._lock = threading.Lock()
        
        atexit.register(self._cleanup)
        
        LOG.info("‚úÖ Environment initialized (mode=%s, obs_dim=%d)", config.mode, config.obs_dim)
    
    def _cleanup(self):
        if not self.closed:
            self.close()
    
    def reset(self, seed: Optional[int] = None) -> np.ndarray:
        with self._lock:
            if seed is not None:
                self.random.seed(seed)
                self.npr.seed(seed)
            
            self.current_time = time.time()
            self.last_action_time = 0.0
            self.step_count = 0
            self.episode_count += 1
            self.closed = False
            self.sla_violations = 0
            self.total_cost = 0.0
            
            # Reset normalizers for new episode
            self.reward_normalizer = RewardNormalizer(clip_range=10.0)
            
            self._reset_state()
            
            obs = self._observe()
            self.metrics_history.clear()
            
            if self.cfg.enable_audit_logging:
                write_audit_event({
                    "event": "env_reset",
                    "episode": self.episode_count,
                    "seed": seed
                })
            
            return obs
    
    def step(self, action: Union[np.ndarray, List, Tuple, Dict]) -> Tuple[np.ndarray, float, bool, Dict]:
        with self._lock:
            self.step_count += 1
            now = time.time()
            self.current_time = now
            
            parsed_action = self._parse_action(action)
            
            if not self._safety_checks(parsed_action):
                parsed_action = {"delta_workers": 0, "throttle": 0.0, "priority_bias": 0}
            
            exec_result = self._apply_action(
                parsed_action["delta_workers"],
                parsed_action["throttle"],
                parsed_action["priority_bias"]
            )
            
            self._advance_world(
                parsed_action["throttle"],
                exec_result.get("applied_delta", 0),
                parsed_action["priority_bias"]
            )
            
            obs = self._observe()
            raw_reward, reward_details = self.compute_reward(exec_result)
            
            # CRITICAL: Normalize reward for stable training
            reward = self.reward_normalizer.normalize(raw_reward)
            
            done = self._terminal_condition()
            
            if self.drift_tracker:
                self.drift_tracker.add_sample(obs, reward)
            
            info = {
                "exec": exec_result,
                "reward_details": reward_details,
                "raw_reward": raw_reward,
                "normalized_reward": reward,
                "step": self.step_count,
                "episode": self.episode_count,
                "timestamp": now,
                "sla_violations": self.sla_violations,
                "total_cost": self.total_cost,
                "circuit_breaker_ok": self.circuit_breaker.can_execute() if self.circuit_breaker else True,
                "drift_score": self.drift_tracker.get_drift_score() if self.drift_tracker else None
            }
            
            self.metrics_history.append({
                "ts": now,
                "action": parsed_action,
                "reward": reward
            })
            
            if parsed_action["delta_workers"] != 0:
                self.last_action_time = now
            
            return obs, float(reward), bool(done), info
    
    def render(self, mode: str = "human") -> Optional[str]:
        output = f"[Step {self.step_count}] SLA violations: {self.sla_violations} Total cost: ${self.total_cost:.2f}"
        if mode == "human":
            print(output)
            return None
        return output
    
    def close(self):
        if self.closed:
            return
        self.closed = True
        
        if self.cfg.enable_audit_logging:
            write_audit_event({
                "event": "env_closed",
                "total_steps": self.step_count,
                "total_episodes": self.episode_count,
                "total_cost": self.total_cost,
                "sla_violations": self.sla_violations
            })
        
        LOG.info("Environment closed (steps=%d, episodes=%d)", self.step_count, self.episode_count)
    
    def _parse_action(self, action: Union[np.ndarray, List, Tuple, Dict]) -> Dict[str, Any]:
        if isinstance(action, dict):
            return {
                "delta_workers": action.get("delta_workers", 0),
                "throttle": action.get("throttle", 0.0),
                "priority_bias": action.get("priority_bias", 0)
            }
        
        arr = np.asarray(action, dtype=float).flatten()
        if arr.size < 3:
            arr = np.pad(arr, (0, 3 - arr.size), constant_values=0)
        
        return {
            "delta_workers": int(np.clip(np.round(arr[0]), -self.cfg.max_scale_delta, self.cfg.max_scale_delta)),
            "throttle": float(np.clip(arr[1], 0.0, 1.0)),
            "priority_bias": int(np.clip(np.round(arr[2]), -2, 2))
        }
    
    def _safety_checks(self, action: Dict[str, Any]) -> bool:
        now = time.time()
        
        if (now - self.last_action_time) < self.cfg.action_cooldown_seconds:
            if action["delta_workers"] != 0:
                return False
        
        if self.circuit_breaker and not self.circuit_breaker.can_execute():
            LOG.warning("‚ö†Ô∏è Circuit breaker OPEN - action blocked")
            return False
        
        return True
    
    def _reset_state(self):
        raise NotImplementedError
    
    def _apply_action(self, delta_workers: int, throttle: float, priority_bias: int) -> Dict[str, Any]:
        raise NotImplementedError
    
    def _advance_world(self, throttle: float, applied_delta: int, priority_bias: int):
        raise NotImplementedError
    
    def _observe(self) -> np.ndarray:
        raise NotImplementedError
    
    def compute_reward(self, exec_result: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        raise NotImplementedError
    
    def _terminal_condition(self) -> bool:
        raise NotImplementedError

# =============================================================================
# SIMULATED ENVIRONMENT (STABLE VERSION)
# =============================================================================

class SimulatedRealEnv(RealEnvBase):
    """
    STABLE PRODUCTION ENVIRONMENT
    ‚úÖ Bounded rewards using tanh()
    ‚úÖ Normalized observations
    ‚úÖ All complex features maintained
    """
    
    def __init__(self, config: EnvConfig):
        super().__init__(config)
        
        self.workload_gen = WorkloadGenerator(config, self.npr)
        self.worker_pool = WorkerPool(config, self.npr)
        
        self.state = {
            "queue_length": 0.0,
            "worker_count": 0.0,
            "healthy_workers": 0.0,
            "avg_latency_ms": 0.0,
            "p95_latency_ms": 0.0,
            "p99_latency_ms": 0.0,
            "success_rate": 1.0,
            "arrival_rate": 0.0,
            "cpu": 0.0,
            "mem": 0.0,
            "cost_rate": 0.0,
            "queue_growth_rate": 0.0
        }
        
        self.recent_latencies = deque(maxlen=1000)
        self.queue_history = deque(maxlen=10)
        self.cascade_failure_active = False
        self.cascade_failure_countdown = 0
        
    def _reset_state(self):
        """Reset with realistic initial conditions"""
        initial_queue = self.npr.randint(200, 800)
        initial_workers = self.npr.randint(10, 30)
        
        self.state.update({
            "queue_length": float(initial_queue),
            "worker_count": float(initial_workers),
            "healthy_workers": float(initial_workers),
            "avg_latency_ms": self.cfg.base_latency_ms * 2.0,
            "p95_latency_ms": self.cfg.base_latency_ms * 3.0,
            "p99_latency_ms": self.cfg.base_latency_ms * 5.0,
            "success_rate": 0.92,
            "arrival_rate": self.cfg.base_arrival_rate,
            "cpu": 0.3,
            "mem": 0.25,
            "cost_rate": 0.0,
            "queue_growth_rate": 0.0
        })
        
        self.worker_pool = WorkerPool(self.cfg, self.npr)
        self.worker_pool.add_workers(initial_workers)
        
        self.workload_gen = WorkloadGenerator(self.cfg, self.npr)
        self.workload_gen.sim_time_minutes = self.npr.uniform(0, 1440)
        self.workload_gen.day_of_week = self.npr.randint(1, 8)
        
        self.recent_latencies.clear()
        self.queue_history.clear()
        self.cascade_failure_active = False
        self.cascade_failure_countdown = 0
        
        LOG.info("üé¨ Episode %d started: queue=%d workers=%d",
                self.episode_count, initial_queue, initial_workers)
    
    def _apply_action(self, delta_workers: int, throttle: float, priority_bias: int) -> Dict[str, Any]:
        """Apply scaling action"""
        attempt = {
            "wanted_delta": delta_workers,
            "applied_delta": 0,
            "throttle": throttle,
            "priority_bias": priority_bias,
            "message": "applied",
            "ts": time.time()
        }
        
        if delta_workers > 0:
            added = self.worker_pool.add_workers(delta_workers)
            attempt["applied_delta"] = added
            if added < delta_workers:
                attempt["message"] = f"hit_max_workers (added {added}/{delta_workers})"
        elif delta_workers < 0:
            removed = self.worker_pool.remove_workers(-delta_workers)
            attempt["applied_delta"] = -removed
            if removed < -delta_workers:
                attempt["message"] = f"hit_min_workers (removed {removed}/{-delta_workers})"
        
        return attempt
    
    def _advance_world(self, throttle: float, applied_delta: int, priority_bias: int):
        """Advance simulation with full complexity"""
        
        # 1. WORKLOAD ARRIVAL
        arrival_rate = self.workload_gen.get_arrival_rate()
        effective_arrival_rate = arrival_rate * (1.0 - throttle)
        arrivals = self.npr.poisson(lam=effective_arrival_rate)
        
        # 2. WORKER POOL UPDATE
        capacity = self.worker_pool.get_effective_capacity()
        overload_factor = max(0.0, (self.state["queue_length"] + arrivals - capacity) / max(1.0, capacity))
        self.worker_pool.update_workers(overload_factor)
        capacity = self.worker_pool.get_effective_capacity()
        
        # 3. QUEUE DYNAMICS
        old_queue = self.state["queue_length"]
        self.state["queue_length"] += arrivals
        
        if capacity > 0:
            completable = min(self.state["queue_length"], capacity)
            
            failure_rate = self.cfg.base_failure_rate
            byzantine_workers = sum(1 for w in self.worker_pool.workers 
                                   if w.failure_mode == FailureMode.BYZANTINE)
            if byzantine_workers > 0:
                failure_rate += 0.05 * (byzantine_workers / max(1, len(self.worker_pool.workers)))
            
            if overload_factor > 2.0:
                failure_rate *= self.cfg.overload_failure_multiplier
            
            if self.cascade_failure_active:
                failure_rate *= 5.0
                self.cascade_failure_countdown -= 1
                if self.cascade_failure_countdown <= 0:
                    self.cascade_failure_active = False
                    LOG.info("‚úÖ Cascade failure recovered")
            
            success_rate = max(0.0, 1.0 - failure_rate)
            successful_completions = int(completable * success_rate)
            
            self.state["queue_length"] -= successful_completions
            self.state["success_rate"] = success_rate
        else:
            self.state["success_rate"] = 0.0
        
        if self.state["queue_length"] >= self.cfg.max_queue_size:
            overflow = self.state["queue_length"] - self.cfg.max_queue_size
            self.state["queue_length"] = self.cfg.max_queue_size
            LOG.warning("‚ö†Ô∏è Queue overflow! Dropped %d tasks", int(overflow))
        
        queue_growth = self.state["queue_length"] - old_queue
        self.queue_history.append(queue_growth)
        if len(self.queue_history) >= 5:
            self.state["queue_growth_rate"] = np.mean(list(self.queue_history))
        else:
            self.state["queue_growth_rate"] = queue_growth
        
        # 4. LATENCY MODELING
        if capacity > 0:
            base_lat = self.cfg.base_latency_ms
            utilization = (arrival_rate / capacity) if capacity > 0 else 10.0
            
            if utilization < 1.0:
                queue_delay = (utilization / (1.0 - utilization)) * base_lat
            else:
                queue_delay = base_lat * 10.0
            
            congestion_delay = self.state["queue_length"] * self.cfg.latency_per_queue_item
            network_delay = self.cfg.network_latency_ms
            total_latency = base_lat + queue_delay + congestion_delay + network_delay
            
            noise = self.npr.randn() * 0.1 * total_latency
            total_latency = max(1.0, total_latency + noise)
            
            slow_workers = sum(1 for w in self.worker_pool.workers 
                              if w.failure_mode == FailureMode.SLOW)
            if slow_workers > 0:
                total_latency *= (1.0 + 0.5 * slow_workers / max(1, len(self.worker_pool.workers)))
            
            self.recent_latencies.append(total_latency)
            
            if len(self.recent_latencies) >= 10:
                sorted_lats = sorted(self.recent_latencies)
                self.state["avg_latency_ms"] = float(np.mean(sorted_lats))
                self.state["p95_latency_ms"] = float(sorted_lats[int(0.95 * len(sorted_lats))])
                self.state["p99_latency_ms"] = float(sorted_lats[int(0.99 * len(sorted_lats))])
            else:
                self.state["avg_latency_ms"] = total_latency
                self.state["p95_latency_ms"] = total_latency * 1.5
                self.state["p99_latency_ms"] = total_latency * 2.0
        else:
            self.state["avg_latency_ms"] = 10000.0
            self.state["p95_latency_ms"] = 20000.0
            self.state["p99_latency_ms"] = 30000.0
        
        # 5. CASCADE FAILURE CHECK
        if not self.cascade_failure_active:
            if self.state["p95_latency_ms"] > self.cfg.sla_latency_p95_ms * 5.0:
                if self.random.random() < self.cfg.cascade_failure_probability:
                    self.cascade_failure_active = True
                    self.cascade_failure_countdown = 20
                    LOG.error("üí• CASCADE FAILURE triggered!")
        
        # 6. RESOURCE UTILIZATION
        worker_count = self.worker_pool.count_total()
        self.state["cpu"] = min(1.0, 0.05 + 0.02 * worker_count + 0.0001 * self.state["queue_length"] + self.npr.randn() * 0.03)
        self.state["mem"] = min(1.0, 0.03 + 0.015 * worker_count + 0.00005 * self.state["queue_length"] + self.npr.randn() * 0.02)
        
        # 7. COST TRACKING
        step_cost = self.worker_pool.get_cost_per_second()
        self.state["cost_rate"] = step_cost
        self.total_cost += step_cost
        
        if self.state["p95_latency_ms"] > self.cfg.sla_latency_p95_ms:
            self.sla_violations += 1
            self.total_cost += self.cfg.cost_sla_violation
        
        # 8. STATE UPDATE
        self.state["worker_count"] = float(worker_count)
        self.state["healthy_workers"] = float(self.worker_pool.count_healthy())
        self.state["arrival_rate"] = arrival_rate
        
        self.workload_gen.advance_time(1.0)
    
    def _observe(self) -> np.ndarray:
        """Return NORMALIZED observation vector"""
        raw_obs = np.array([
            self.state["queue_length"],
            self.state["worker_count"],
            self.state["avg_latency_ms"],
            self.state["p95_latency_ms"],
            self.state["success_rate"],
            self.state["arrival_rate"],
            self.state["cpu"],
            self.state["mem"],
            self.state["p99_latency_ms"],
            self.state["cost_rate"],
            self.state["healthy_workers"] / max(1.0, self.state["worker_count"]),
            self.state["queue_growth_rate"]
        ], dtype=np.float32)
        
        # Match configured obs_dim
        if self.cfg.obs_dim and len(raw_obs) < self.cfg.obs_dim:
            raw_obs = np.pad(raw_obs, (0, self.cfg.obs_dim - len(raw_obs)), constant_values=0)
        elif self.cfg.obs_dim and len(raw_obs) > self.cfg.obs_dim:
            raw_obs = raw_obs[:self.cfg.obs_dim]
        
        # CRITICAL: Normalize observations
        normalized_obs = self.obs_normalizer.normalize(raw_obs)
        
        return normalized_obs
    
    def compute_reward(self, exec_result: Dict[str, Any]) -> Tuple[float, Dict[str, Any]]:
        """
        STABLE REWARD FUNCTION with tanh() bounded components
        ‚úÖ All components bounded to [-1, 1] range
        ‚úÖ Prevents value explosion
        ‚úÖ Maintains all objectives
        """
        
        p95_lat = self.state["p95_latency_ms"]
        succ_rate = self.state["success_rate"]
        queue = self.state["queue_length"]
        workers = self.state["worker_count"]
        healthy_ratio = self.state["healthy_workers"] / max(1.0, workers)
        cost_rate = self.state["cost_rate"]
        delta = exec_result.get("applied_delta", 0)
        
        # === COMPONENT 1: LATENCY (bounded with tanh) ===
        p95_target = self.cfg.sla_latency_p95_ms
        overage_ratio = (p95_lat - p95_target) / (p95_target + 1e-8)
        # tanh bounds to [-1, 1]
        latency_reward = -np.tanh(overage_ratio) * self.cfg.reward_latency_weight
        
        # === COMPONENT 2: AVAILABILITY (bounded with tanh) ===
        deficit = (self.cfg.sla_success_rate - succ_rate) / (self.cfg.sla_success_rate + 1e-8)
        availability_reward = -np.tanh(deficit * 3.0) * self.cfg.reward_availability_weight
        
        # === COMPONENT 3: COST EFFICIENCY (bounded with tanh) ===
        successful_throughput = workers * healthy_ratio * succ_rate
        if successful_throughput > 0:
            cost_per_success = cost_rate / successful_throughput
            cost_ratio = cost_per_success / 0.001  # Normalize by expected cost
            cost_penalty = -np.tanh(cost_ratio - 1.0) * self.cfg.reward_cost_weight
        else:
            cost_penalty = -self.cfg.reward_cost_weight
        
        # === COMPONENT 4: QUEUE MANAGEMENT (bounded with tanh) ===
        queue_ratio = queue / (self.cfg.max_queue_size + 1e-8)
        queue_reward = -np.tanh((queue_ratio - 0.3) * 3.0)
        
        # === COMPONENT 5: EFFICIENCY (bounded with tanh) ===
        utilization = queue / max(1.0, workers * 10.0)
        efficiency_reward = -np.tanh(abs(utilization - 0.65) * 2.0) * self.cfg.reward_efficiency_weight
        
        # === COMPONENT 6: STABILITY (bounded) ===
        stability_penalty = -np.tanh(abs(delta) / 10.0) * self.cfg.reward_stability_weight
        
        # === COMPONENT 7: SLA BONUS (bounded) ===
        sla_bonus = 0.0
        if (p95_lat <= p95_target and 
            succ_rate >= self.cfg.sla_success_rate and
            queue_ratio < 0.5):
            sla_bonus = self.cfg.reward_sla_bonus
        
        # === AGGREGATE REWARD (naturally bounded) ===
        total_reward = (
            latency_reward +
            availability_reward +
            cost_penalty +
            queue_reward +
            efficiency_reward +
            stability_penalty +
            sla_bonus
        )
        
        # === SAFETY CLIP (should rarely trigger) ===
        total_reward = float(np.clip(total_reward, -20.0, 20.0))
        
        # === CIRCUIT BREAKER UPDATE ===
        if self.circuit_breaker:
            if succ_rate > 0.9 and p95_lat < p95_target * 2.0:
                self.circuit_breaker.record_success()
            else:
                self.circuit_breaker.record_failure()
        
        # === DETAILS ===
        details = {
            "latency_reward": float(latency_reward),
            "availability_reward": float(availability_reward),
            "cost_penalty": float(cost_penalty),
            "queue_reward": float(queue_reward),
            "efficiency_reward": float(efficiency_reward),
            "stability_penalty": float(stability_penalty),
            "sla_bonus": float(sla_bonus),
            "final_reward": total_reward,
            "p95_sla_met": p95_lat <= p95_target,
            "success_sla_met": succ_rate >= self.cfg.sla_success_rate
        }
        
        return total_reward, details
    
    def _terminal_condition(self) -> bool:
        """Lenient termination for stable training"""
        MAX_STEPS = 2000  # Match steps_per_epoch from config
        
        if self.step_count >= MAX_STEPS:
            LOG.info("Episode complete: reached max steps")
            return True
        
        # Only terminate on catastrophic failures
        if self.state["success_rate"] < 0.01 and self.state["worker_count"] < 5:
            LOG.error("üíÄ Episode terminated: complete system failure")
            return True
        
        if (self.state["queue_length"] >= self.cfg.max_queue_size * 0.999 and
            self.state["queue_growth_rate"] > 100):
            LOG.error("üíÄ Episode terminated: queue overflow")
            return True
        
        if (self.state["p99_latency_ms"] > 100000 and
            self.state["queue_length"] > self.cfg.max_queue_size * 0.9):
            LOG.error("üíÄ Episode terminated: extreme latency")
            return True
        
        return False

# =============================================================================
# FACTORY FUNCTIONS
# =============================================================================

def make_env(cfg: EnvConfig) -> RealEnvBase:
    """Factory to create environment"""
    return SimulatedRealEnv(cfg)

def make_vec_env(cfg: EnvConfig, n: int = 4) -> List[RealEnvBase]:
    """Create vectorized environments"""
    envs = []
    base_seed = cfg.seed if cfg.seed is not None else int(time.time())
    
    for i in range(n):
        env_cfg = EnvConfig(**asdict(cfg))
        env_cfg.seed = base_seed + i
        envs.append(make_env(env_cfg))
    
    return envs

# =============================================================================
# DIAGNOSTICS
# =============================================================================

def run_env_diagnostic(cfg: Optional[EnvConfig] = None) -> Dict[str, Any]:
    """Run comprehensive environment diagnostic"""
    if cfg is None:
        cfg = EnvConfig(seed=42)
    
    results = {
        "config": {k: str(v) if isinstance(v, Enum) else v for k, v in asdict(cfg).items()},
        "tests": {}
    }
    
    try:
        env = make_env(cfg)
        results["tests"]["create_env"] = "‚úÖ PASS"
        
        obs = env.reset(seed=42)
        results["tests"]["reset"] = f"‚úÖ PASS (obs_shape={obs.shape}, obs_range=[{obs.min():.2f}, {obs.max():.2f}])"
        
        # Test episode
        total_reward = 0.0
        raw_rewards = []
        for step in range(100):
            action = np.random.randn(3) * 5
            action[1] = np.clip(action[1], 0, 1)
            obs, reward, done, info = env.step(action)
            total_reward += reward
            raw_rewards.append(info['raw_reward'])
            
            if done:
                break
        
        results["tests"]["episode"] = f"‚úÖ PASS (steps={step+1}, reward={total_reward:.2f}, raw_range=[{min(raw_rewards):.2f}, {max(raw_rewards):.2f}])"
        
        # Test scaling
        obs = env.reset(seed=43)
        obs, reward, done, info = env.step(np.array([10.0, 0.0, 0.0]))
        results["tests"]["scale_up"] = f"‚úÖ PASS (delta={info['exec']['applied_delta']}, reward={reward:.2f})"
        
        obs, reward, done, info = env.step(np.array([-5.0, 0.0, 0.0]))
        results["tests"]["scale_down"] = f"‚úÖ PASS (delta={info['exec']['applied_delta']}, reward={reward:.2f})"
        
        # Test reward bounds
        obs = env.reset(seed=44)
        rewards = []
        for _ in range(50):
            action = np.random.randn(3) * 10
            action[1] = np.clip(action[1], 0, 1)
            obs, reward, done, info = env.step(action)
            rewards.append(reward)
            if done:
                break
        
        results["tests"]["reward_bounds"] = f"‚úÖ PASS (range=[{min(rewards):.2f}, {max(rewards):.2f}], mean={np.mean(rewards):.2f})"
        
        env.close()
        results["tests"]["close"] = "‚úÖ PASS"
        
        results["status"] = "‚úÖ ALL TESTS PASSED"
        
    except Exception as e:
        results["status"] = f"‚ùå FAILED: {e}"
        results["error"] = str(e)
        import traceback
        results["traceback"] = traceback.format_exc()
    
    return results

# =============================================================================
# CLI
# =============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="PriorityMax STABLE Environment")
    parser.add_argument("--diagnostic", action="store_true", help="Run diagnostic tests")
    parser.add_argument("--workload", default="ecommerce", choices=[w.value for w in WorkloadType])
    parser.add_argument("--steps", type=int, default=100, help="Interactive test steps")
    parser.add_argument("--verbose", action="store_true")
    
    args = parser.parse_args()
    
    if args.verbose:
        LOG.setLevel(logging.DEBUG)
    
    if args.diagnostic:
        print("\n" + "="*80)
        print("PriorityMax STABLE Environment - Diagnostic Report")
        print("="*80 + "\n")
        
        cfg = EnvConfig(workload_type=args.workload, seed=42)
        results = run_env_diagnostic(cfg)
        
        print(json.dumps(results, indent=2))
        print("\n" + "="*80)
        print(f"Status: {results['status']}")
        print("="*80 + "\n")
        
        sys.exit(0 if results["status"].startswith("‚úÖ") else 1)
    
    else:
        print(f"\nüéÆ Interactive STABLE Environment Test ({args.workload})\n")
        
        cfg = EnvConfig(workload_type=args.workload, seed=42)
        env = make_env(cfg)
        
        obs = env.reset()
        print(f"Initial observation shape: {obs.shape}")
        print(f"Observation range: [{obs.min():.2f}, {obs.max():.2f}]")
        print(f"Action space: {env.action_space}\n")
        
        for step in range(args.steps):
            action = np.random.randn(3) * 5
            action[1] = np.clip(action[1], 0, 1)
            
            obs, reward, done, info = env.step(action)
            
            if step % 20 == 0:
                print(f"\n{'='*60}")
                print(f"Step {step}:")
                print(f"  Normalized obs range: [{obs.min():.2f}, {obs.max():.2f}]")
                print(f"  Normalized reward: {reward:.2f} | Raw reward: {info['raw_reward']:.2f}")
                print(f"  SLA violations: {info['sla_violations']} | Cost: ${info['total_cost']:.2f}")
            
            if done:
                print(f"\n‚ùå Episode terminated at step {step}")
                break
        
        env.close()
        print("\n‚úÖ Test complete!\n")